{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups"
      ],
      "metadata": {
        "id": "yRSqp6uOikGe"
      },
      "id": "yRSqp6uOikGe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae5e6929",
      "metadata": {
        "id": "ae5e6929"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import string\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb61521c",
      "metadata": {
        "id": "cb61521c"
      },
      "outputs": [],
      "source": [
        "main_path = '20_newsgroups'\n",
        "folders = listdir(main_path)\n",
        "\n",
        "documents = []\n",
        "Y = []\n",
        "\n",
        "for folder in folders:\n",
        "    folder_path = join(main_path, folder)\n",
        "    total_files_in_folder = len(listdir(folder_path))\n",
        "    Y += ([folder] * total_files_in_folder)\n",
        "    for document in listdir(folder_path):\n",
        "        documents.append(join(folder_path, document))\n",
        "        \n",
        "documents = np.array(documents)\n",
        "Y = np.array(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7565ba55",
      "metadata": {
        "id": "7565ba55"
      },
      "outputs": [],
      "source": [
        "document_train, document_test, Y_train, Y_test = np.array([]), np.array([]), np.array([]), np.array([])\n",
        "for folder in folders:  \n",
        "    a, b, c, d = train_test_split(documents[Y == folder], Y[Y == folder], random_state = 0,\n",
        "                                  test_size = 0.25)\n",
        "    document_train = np.append(document_train, a) \n",
        "    document_test = np.append(document_test, b)\n",
        "    Y_train = np.append(Y_train, c)\n",
        "    Y_test = np.append(Y_test, d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbfd5984",
      "metadata": {
        "id": "bbfd5984"
      },
      "outputs": [],
      "source": [
        "#function to preprocess the words list to remove punctuations\n",
        "def preprocess(words_list):\n",
        "    #we create an empty translation table where every character in the first argument is mapped to\n",
        "    #every character in the second argument and every character in the third argument is mapped to\n",
        "    #none  \n",
        "    \n",
        "    #\" \\t \" in a word becomes none\n",
        "    translation_table = str.maketrans('', '', '\\t')\n",
        "    words_list = [word.translate(translation_table) for word in words_list]\n",
        "    \n",
        "    #\" ' \" appears in a lot of words and would change the meaning of the words if removed,\n",
        "    #hence it is removed from the list of punctuations we plan to remove from the words\n",
        "    punctuations = (string.punctuation).replace(\"'\", \"\") \n",
        "    #all punctuation characters become none \n",
        "    translation_table = str.maketrans('', '', punctuations)\n",
        "    words_list = [word.translate(translation_table) for word in words_list]\n",
        "    \n",
        "    #removing blank strings\n",
        "    words_list = [word for word in words_list if word]\n",
        "    \n",
        "    #some words are quoted in the documents and as we have not removed \" ' \" to maintain \n",
        "    #the meaning of the words, we try to unquote such words below\n",
        "    for i in range(len(words_list)):\n",
        "        if ((words_list[i][0] == \"'\") and (words_list[i][-1] == \"'\")):\n",
        "            words_list[i] = words_list[i][1:-1]\n",
        "        elif(words_list[i][0] == \"'\"):\n",
        "            words_list[i] = words_list[i][1:]\n",
        "        \n",
        "    #we will also remove just numeric strings as they do not have any significant meaning in \n",
        "    #text classification\n",
        "    words_list = [word for word in words_list if not word.isdigit()]\n",
        "    \n",
        "    #removing blank strings\n",
        "    words_list = [word for word in words_list if word]\n",
        "    \n",
        "    #making all words lower-case\n",
        "    words_list = [word.lower() for word in words_list]\n",
        "    \n",
        "    #removing words with two or less characters\n",
        "    words_list = [word for word in words_list if (len(word) > 2)]\n",
        "    \n",
        "    return words_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eae0b91",
      "metadata": {
        "id": "0eae0b91"
      },
      "outputs": [],
      "source": [
        "#function to remove stopwords\n",
        "stopwords = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any',\n",
        "             'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', \n",
        "             'between', 'both', 'but', 'by', 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'did',\n",
        "             \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few',\n",
        "             'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", \n",
        "             'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself',\n",
        "             'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', \n",
        "             'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', \n",
        "             'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', \n",
        "             'only', 'or', 'other', 'ought', 'our', 'ours' 'ourselves', 'out', 'over', 'own',\n",
        "             'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', \n",
        "             'some', 'such', 'than', 'that',\"that's\", 'the', 'their', 'theirs', 'them', \n",
        "             'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \n",
        "             \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', \n",
        "             'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \n",
        "             \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while',\n",
        "             'who', \"who's\", 'whom', 'why', \"why's\",'will', 'with', \"won't\", 'would', \"wouldn't\",\n",
        "             'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', \n",
        "             'yourselves', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine',\n",
        "             'ten', 'hundred', 'thousand', '1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th',\n",
        "             '9th', '10th']\n",
        "\n",
        "def remove_stopwords(words_list):\n",
        "    words_list = [word for word in words_list if not word in stopwords]\n",
        "    return words_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e80bd15c",
      "metadata": {
        "id": "e80bd15c"
      },
      "outputs": [],
      "source": [
        "#function to convert a single line into a list of words\n",
        "def tokenize_line(line):\n",
        "    words_list = line[0:].strip().split(\" \")\n",
        "    words_list = preprocess(words_list)\n",
        "    words_list = remove_stopwords(words_list)\n",
        "    \n",
        "    return words_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51c51381",
      "metadata": {
        "id": "51c51381"
      },
      "outputs": [],
      "source": [
        "#function to remove metadata from the whole document\n",
        "def remove_metadata(lines):\n",
        "    for i in range(len(lines)):\n",
        "        if(lines[i] == '\\n'):\n",
        "            temp = i + 1\n",
        "            break\n",
        "    lines = lines[temp:]\n",
        "    return lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69173b39",
      "metadata": {
        "id": "69173b39"
      },
      "outputs": [],
      "source": [
        "#function to convert a document into a list of words\n",
        "def tokenize_document(path):\n",
        "    #load the document as a list of lines\n",
        "    f = open(path, 'r')\n",
        "    document_lines = f.readlines()\n",
        "    \n",
        "    #removing the metadata at the top of each document\n",
        "    document_lines = remove_metadata(document_lines)\n",
        "    \n",
        "    document_words = []\n",
        "    \n",
        "    #tokenize all the lines of the document\n",
        "    for line in document_lines:\n",
        "        document_words.append(tokenize_line(line))\n",
        "\n",
        "    return document_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e903496b",
      "metadata": {
        "id": "e903496b"
      },
      "outputs": [],
      "source": [
        "#function to flatten a 2D array to a 1D array\n",
        "def flatten(words_list):\n",
        "    words_list_flattened = []\n",
        "    for line in words_list:\n",
        "        for word in line:\n",
        "            words_list_flattened.append(word)\n",
        "    return words_list_flattened"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67e50717",
      "metadata": {
        "id": "67e50717"
      },
      "outputs": [],
      "source": [
        "training_document_words_list = []\n",
        "for document in document_train:\n",
        "    training_document_words_list.append(flatten(tokenize_document(document)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0b1bed6",
      "metadata": {
        "id": "d0b1bed6"
      },
      "outputs": [],
      "source": [
        "all_words_list = np.array(flatten(training_document_words_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29b164ad",
      "metadata": {
        "id": "29b164ad",
        "outputId": "87371401-9100-45de-f1be-9418f8f90500"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1917496"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_words_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80c038b3",
      "metadata": {
        "id": "80c038b3"
      },
      "outputs": [],
      "source": [
        "#finding frequencies of unique words and sorting them in decreasing order\n",
        "unique_words, frequency = np.unique(all_words_list, return_counts = True)\n",
        "unique_words_sorted, frequency_sorted = (list(i) for i in zip(*(sorted(zip(unique_words, frequency), \n",
        "                                        key = lambda awd:(awd[1], awd[0]), reverse=True))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c6d4e1e",
      "metadata": {
        "id": "3c6d4e1e"
      },
      "outputs": [],
      "source": [
        "#making variables for frequency of words and number of words with that frequency\n",
        "number_of_words = []\n",
        "frequency_of_words = []\n",
        "for specific_frequency in sorted(np.unique(frequency_sorted), reverse=True):\n",
        "    frequency_of_words.append(specific_frequency)\n",
        "    number_of_words.append(frequency_sorted.count(specific_frequency))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3811c447",
      "metadata": {
        "id": "3811c447",
        "outputId": "b486a6db-628d-4bfd-d581-8e1e50488c2a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#plotting graph for frequency vs number of words with that frequency\n",
        "x = number_of_words\n",
        "y = frequency_of_words\n",
        "plt.xlim(0, 100)\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.plot(x, y)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abb9f47b",
      "metadata": {
        "id": "abb9f47b"
      },
      "outputs": [],
      "source": [
        "#using first 5000 words as features\n",
        "features = unique_words_sorted[0:5000]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96f1aa11",
      "metadata": {
        "id": "96f1aa11"
      },
      "source": [
        "# Making X_train and Y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc7869fc",
      "metadata": {
        "id": "bc7869fc"
      },
      "outputs": [],
      "source": [
        "#making the final X_train 2d array\n",
        "\n",
        "#making a dictionary with count of unique words for every document\n",
        "X_train_dictionary = {}\n",
        "document_number = 1\n",
        "for document_words in training_document_words_list:\n",
        "    document_words = np.asarray(document_words)\n",
        "    unique_words, frequency = np.unique(document_words, return_counts = True)\n",
        "    X_train_dictionary[document_number] = {}\n",
        "    for i in range(len(unique_words)):\n",
        "        X_train_dictionary[document_number][unique_words[i]] = frequency[i]\n",
        "    document_number = document_number + 1\n",
        "    \n",
        "#converting dictionary to X_train\n",
        "X_train = []\n",
        "for document_number in X_train_dictionary.keys():\n",
        "    X_train_row = []\n",
        "    for word in features:\n",
        "        if(word in X_train_dictionary[document_number].keys()):\n",
        "            X_train_row.append(X_train_dictionary[document_number][word]) \n",
        "        else:\n",
        "            X_train_row.append(0)\n",
        "    X_train.append(X_train_row)\n",
        "    \n",
        "X_train = np.asarray(X_train)\n",
        "Y_train = np.asarray(Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47c430ef",
      "metadata": {
        "id": "47c430ef"
      },
      "source": [
        "# Making X_test and Y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd3ab34c",
      "metadata": {
        "id": "bd3ab34c"
      },
      "outputs": [],
      "source": [
        "#making the list of words for every testing document\n",
        "testing_document_words_list = []\n",
        "for document in document_test:\n",
        "    testing_document_words_list.append(flatten(tokenize_document(document)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "905bbede",
      "metadata": {
        "id": "905bbede"
      },
      "outputs": [],
      "source": [
        "#making the final X_test 2d array\n",
        "\n",
        "#making a dictionary with count of unique words for every document\n",
        "X_test_dictionary = {}\n",
        "document_number = 1\n",
        "for document_words in testing_document_words_list:\n",
        "    document_words = np.asarray(document_words)\n",
        "    unique_words, frequency = np.unique(document_words, return_counts = True)\n",
        "    X_test_dictionary[document_number] = {}\n",
        "    for i in range(len(unique_words)):\n",
        "        X_test_dictionary[document_number][unique_words[i]] = frequency[i]\n",
        "    document_number = document_number + 1\n",
        "    \n",
        "#converting dictionary to X_test\n",
        "X_test = []\n",
        "for document_number in X_test_dictionary.keys():\n",
        "    X_test_row = []\n",
        "    for word in features:\n",
        "        if(word in X_test_dictionary[document_number].keys()):\n",
        "            X_test_row.append(X_test_dictionary[document_number][word]) \n",
        "        else:\n",
        "            X_test_row.append(0)\n",
        "    X_test.append(X_test_row)\n",
        "    \n",
        "X_test = np.asarray(X_test)\n",
        "Y_test = np.asarray(Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5895cf34",
      "metadata": {
        "id": "5895cf34"
      },
      "source": [
        "# Text Classification using Multinomial Naive Bayes from SKLearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e51bd31d",
      "metadata": {
        "id": "e51bd31d"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55ced338",
      "metadata": {
        "id": "55ced338",
        "outputId": "5b58b6b5-060e-45ce-8dac-ab663ed0786e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "217556d4",
      "metadata": {
        "id": "217556d4"
      },
      "source": [
        "#### Testing Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4608eca5",
      "metadata": {
        "id": "4608eca5"
      },
      "outputs": [],
      "source": [
        "Y_test_preds = classifier.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f981056",
      "metadata": {
        "id": "6f981056",
        "outputId": "bed4e193-7575-4184-d7b4-5a23dbed67f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.61      0.74      0.67       250\n",
            "           comp.graphics       0.63      0.64      0.64       250\n",
            " comp.os.ms-windows.misc       0.68      0.70      0.69       250\n",
            "comp.sys.ibm.pc.hardware       0.66      0.66      0.66       250\n",
            "   comp.sys.mac.hardware       0.72      0.78      0.75       250\n",
            "          comp.windows.x       0.79      0.75      0.77       250\n",
            "            misc.forsale       0.79      0.74      0.76       250\n",
            "               rec.autos       0.82      0.88      0.85       250\n",
            "         rec.motorcycles       0.83      0.89      0.86       250\n",
            "      rec.sport.baseball       0.90      0.91      0.91       250\n",
            "        rec.sport.hockey       0.94      0.93      0.93       250\n",
            "               sci.crypt       0.88      0.88      0.88       250\n",
            "         sci.electronics       0.77      0.68      0.72       250\n",
            "                 sci.med       0.91      0.82      0.87       250\n",
            "               sci.space       0.85      0.82      0.84       250\n",
            "  soc.religion.christian       0.74      0.82      0.78       250\n",
            "      talk.politics.guns       0.72      0.82      0.77       250\n",
            "   talk.politics.mideast       0.90      0.84      0.87       250\n",
            "      talk.politics.misc       0.64      0.63      0.64       250\n",
            "      talk.religion.misc       0.52      0.36      0.42       250\n",
            "\n",
            "                accuracy                           0.77      5000\n",
            "               macro avg       0.77      0.77      0.76      5000\n",
            "            weighted avg       0.77      0.77      0.76      5000\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------\n",
            "Confusion Matrix:\n",
            " [[186   0   0   0   0   0   0   2   1   0   0   2   2   1   4  14   2   7\n",
            "    3  26]\n",
            " [  3 160  28  13   8  17   3   1   2   0   1   1   3   2   3   0   1   0\n",
            "    2   2]\n",
            " [  1  14 174  19   7  20   5   0   1   2   0   2   3   0   1   1   0   0\n",
            "    0   0]\n",
            " [  3   8  20 166  27   3   8   2   2   0   0   1   9   0   1   0   0   0\n",
            "    0   0]\n",
            " [  0   4   8  17 196   0   5   2   1   0   0   2   9   2   3   0   0   0\n",
            "    1   0]\n",
            " [  4  28  15   3   1 188   4   1   1   0   0   1   1   0   3   0   0   0\n",
            "    0   0]\n",
            " [  4   4   4  12  11   1 185   9   4   2   1   4   5   0   2   1   1   0\n",
            "    0   0]\n",
            " [  1   0   0   2   0   2   4 221   8   0   0   1   4   2   0   0   2   0\n",
            "    3   0]\n",
            " [  2   1   0   1   3   0   2  11 222   2   1   0   1   1   0   0   2   0\n",
            "    1   0]\n",
            " [  0   2   0   0   1   1   3   2   3 228   8   0   1   0   0   1   0   0\n",
            "    0   0]\n",
            " [  0   0   2   0   0   0   1   0   2   8 232   3   0   0   0   1   0   0\n",
            "    1   0]\n",
            " [  2   4   1   0   3   1   1   1   0   1   0 220   4   0   2   0   3   0\n",
            "    5   2]\n",
            " [  1  14   1  16  15   4   7   8   0   1   0   3 169   3   3   1   0   1\n",
            "    3   0]\n",
            " [  5   3   1   1   0   0   1   1   9   2   1   1   4 206   3   5   2   1\n",
            "    4   0]\n",
            " [  3   7   0   0   0   1   0   4   3   1   1   0   4   4 206   2   0   0\n",
            "   11   3]\n",
            " [ 12   1   1   0   1   0   1   0   2   2   2   1   0   1   1 205   2   1\n",
            "    4  13]\n",
            " [  1   0   0   0   0   1   2   1   3   1   0   4   0   0   0   0 206   0\n",
            "   21  10]\n",
            " [  4   0   0   0   0   0   2   3   2   0   0   2   0   0   0   4   4 211\n",
            "   15   3]\n",
            " [  3   0   0   1   0   0   0   1   2   2   0   2   0   3   6   1  38  10\n",
            "  157  24]\n",
            " [ 71   2   0   0   0   0   0   0   1   0   0   0   1   1   5  40  24   3\n",
            "   13  89]]\n",
            "------------------------------------------------------------------------------------------------------------------------------------\n",
            "Accuracy: 76.53999999999999 %\n"
          ]
        }
      ],
      "source": [
        "print('Classification Report:\\n', classification_report(Y_test, Y_test_preds))\n",
        "print('-'*132)\n",
        "print('Confusion Matrix:\\n', confusion_matrix(Y_test, Y_test_preds))\n",
        "print('-'*132)\n",
        "print('Accuracy:', accuracy_score(Y_test, Y_test_preds) * 100, '%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8d828e1",
      "metadata": {
        "id": "d8d828e1"
      },
      "source": [
        "#### Training Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "459c10aa",
      "metadata": {
        "id": "459c10aa"
      },
      "outputs": [],
      "source": [
        "Y_train_preds = classifier.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e354b276",
      "metadata": {
        "id": "e354b276",
        "outputId": "f39996f0-aa62-4987-95d6-f4f4d62437e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.69      0.86      0.77       750\n",
            "           comp.graphics       0.70      0.73      0.72       750\n",
            " comp.os.ms-windows.misc       0.79      0.81      0.80       750\n",
            "comp.sys.ibm.pc.hardware       0.77      0.79      0.78       750\n",
            "   comp.sys.mac.hardware       0.80      0.86      0.83       750\n",
            "          comp.windows.x       0.86      0.82      0.84       750\n",
            "            misc.forsale       0.84      0.84      0.84       750\n",
            "               rec.autos       0.87      0.88      0.88       750\n",
            "         rec.motorcycles       0.89      0.95      0.92       750\n",
            "      rec.sport.baseball       0.93      0.94      0.93       750\n",
            "        rec.sport.hockey       0.97      0.96      0.96       750\n",
            "               sci.crypt       0.94      0.89      0.91       750\n",
            "         sci.electronics       0.84      0.78      0.81       750\n",
            "                 sci.med       0.91      0.90      0.91       750\n",
            "               sci.space       0.94      0.87      0.90       750\n",
            "  soc.religion.christian       0.85      0.90      0.87       747\n",
            "      talk.politics.guns       0.74      0.88      0.80       750\n",
            "   talk.politics.mideast       0.92      0.86      0.89       750\n",
            "      talk.politics.misc       0.75      0.67      0.71       750\n",
            "      talk.religion.misc       0.73      0.50      0.59       750\n",
            "\n",
            "                accuracy                           0.83     14997\n",
            "               macro avg       0.84      0.83      0.83     14997\n",
            "            weighted avg       0.84      0.83      0.83     14997\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------\n",
            "Confusion Matrix:\n",
            " [[647   2   1   0   0   1   0   2   5   1   0   0   2   7   1  26   4   6\n",
            "    4  41]\n",
            " [  6 548  50  33  24  34   8   7   2   2   1   4   9  11   6   0   3   0\n",
            "    0   2]\n",
            " [  8  31 607  44   7  26   3   3   1   2   1   1   5   1   2   2   0   1\n",
            "    0   5]\n",
            " [  1  17  36 594  45   6  19   6   1   2   0   2  19   0   1   0   0   1\n",
            "    0   0]\n",
            " [  8   8  17  33 644   5  16   2   2   0   0   0   9   5   0   0   0   0\n",
            "    1   0]\n",
            " [  2  62  18   6  10 617   7   0   1   1   1   3   7   1   7   0   1   0\n",
            "    3   3]\n",
            " [  7   2  10  27  17   1 628  16   5   4   0   2  15   1   6   1   3   0\n",
            "    5   0]\n",
            " [  5   3   1   4   4   1  18 661  14   5   1   2  17   1   2   1   7   0\n",
            "    1   2]\n",
            " [  2   5   0   0   1   0   7  13 711   2   1   0   3   3   0   0   1   0\n",
            "    0   1]\n",
            " [  4   4   3   0   1   2  10   3   7 703   8   0   1   2   1   0   1   0\n",
            "    0   0]\n",
            " [  2   2   0   1   0   1   1   2   6  10 717   0   1   2   0   1   0   0\n",
            "    2   2]\n",
            " [  4  15   7   0   6   4   0   2   5   1   1 666   3   2   2   0  12   1\n",
            "   18   1]\n",
            " [  1  25   6  28  36   4  17  13   8   4   4   7 586   5   2   1   1   0\n",
            "    1   1]\n",
            " [ 12  14   1   1   5   2   2   4   7   2   2   0  10 676   1   2   4   3\n",
            "    2   0]\n",
            " [  7  26   1   0   1   5   2   5   6   4   0   2   6   7 656   4   3   1\n",
            "   10   4]\n",
            " [ 25   2   4   2   0   1   4   0   2   2   0   0   3   4   1 671   7   5\n",
            "    5   9]\n",
            " [  3   0   0   0   0   3   5   9   2   2   2   7   2   3   1   2 660   3\n",
            "   34  12]\n",
            " [ 11   6   0   1   1   1   3   1   5   3   2   4   0   3   4   7   6 648\n",
            "   39   5]\n",
            " [ 15   4   1   1   1   0   1   5   3   4   1   6   0   2   6   5 110  34\n",
            "  499  52]\n",
            " [165   2   5   1   0   0   1   4   7   1   1   1   1   6   1  67  70   4\n",
            "   40 373]]\n",
            "------------------------------------------------------------------------------------------------------------------------------------\n",
            "Accuracy: 83.43001933720078 %\n"
          ]
        }
      ],
      "source": [
        "print('Classification Report:\\n', classification_report(Y_train, Y_train_preds))\n",
        "print('-'*132)\n",
        "print('Confusion Matrix:\\n', confusion_matrix(Y_train, Y_train_preds))\n",
        "print('-'*132)\n",
        "print('Accuracy:', accuracy_score(Y_train, Y_train_preds) * 100, '%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "457dd5bf",
      "metadata": {
        "id": "457dd5bf"
      },
      "source": [
        "# Text Classification using Multinomial Naive Bayes from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "771c4b54",
      "metadata": {
        "id": "771c4b54"
      },
      "outputs": [],
      "source": [
        "#The Multinomial Naive Bayes class.\n",
        "class MultinomialNaiveBayes:\n",
        "    '''This class implements the Multinomial Naive Bayes classifier.'''\n",
        "\n",
        "    #Constructor which initializes the dictionary.\n",
        "    def __init__(self):\n",
        "        self.X_train_dictionary = {}\n",
        "\n",
        "    #The fit function analogous to the one in Scikit Learn. This function creates the dictionary\n",
        "    #which contains the frequency of every word in the vocabulary per class along with class length\n",
        "    #per class and total length\n",
        "    def fit(self, X_train, Y_train):\n",
        "        self.classes = np.unique(Y_train)\n",
        "        self.X_train_dictionary['total_length'] = len(X_train)\n",
        "        for cls in self.classes:\n",
        "            current_class_X_train = X_train[Y_train == cls]\n",
        "            self.X_train_dictionary[cls] = {}\n",
        "            self.X_train_dictionary[cls]['total_words'] = 0\n",
        "            self.X_train_dictionary[cls]['class_length'] = len(current_class_X_train)\n",
        "            for i in range(X_train.shape[1]):\n",
        "                word_frequency = current_class_X_train[:, i].sum()\n",
        "                self.X_train_dictionary[cls][i] = word_frequency\n",
        "                self.X_train_dictionary[cls]['total_words'] += word_frequency\n",
        "        \n",
        "    #this function calculates the probability of a single test data point belonging to a single class\n",
        "    def probability(self, x, current_class):\n",
        "        result = np.log(self.X_train_dictionary[current_class]['class_length']) - np.log(self.X_train_dictionary['total_length'])\n",
        "        for i in range(len(x)):\n",
        "            if(x[i] == 0):\n",
        "                continue\n",
        "            numerator = self.X_train_dictionary[current_class][i] + 1\n",
        "            denominator = self.X_train_dictionary[current_class]['total_words'] + len(x)\n",
        "            result += (x[i]*(np.log(numerator) - np.log(denominator)))\n",
        "\n",
        "        return result\n",
        "    \n",
        "    #this is a helper function used to predict the class for a single test data point \n",
        "    def predict_single_point(self, x):\n",
        "        best_probability = -100000000\n",
        "        best_class = -1\n",
        "\n",
        "        for current_class in self.classes:\n",
        "            current_class_probability = self.probability(x, current_class)\n",
        "            if(current_class_probability > best_probability):\n",
        "                best_probability = current_class_probability\n",
        "                best_class = current_class \n",
        "    \n",
        "        return best_class\n",
        "    \n",
        "    #this function is analogous to the predict function in scikit learn which takes in X_test and returns\n",
        "    #the predictions for the same\n",
        "    def predict(self, X_test):\n",
        "        Y_preds = []\n",
        "        for x in X_test:\n",
        "            Y_preds.append(self.predict_single_point(x))\n",
        "        return Y_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8695678",
      "metadata": {
        "id": "d8695678"
      },
      "source": [
        "#### Testing Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1efadff4",
      "metadata": {
        "id": "1efadff4"
      },
      "outputs": [],
      "source": [
        "classifier = MultinomialNaiveBayes()\n",
        "classifier.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8be9cc45",
      "metadata": {
        "id": "8be9cc45"
      },
      "outputs": [],
      "source": [
        "Y_preds = classifier.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc7a825f",
      "metadata": {
        "id": "cc7a825f",
        "outputId": "cc8cac1a-795f-4eaf-db35-8208afe18bdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.61      0.74      0.67       250\n",
            "           comp.graphics       0.63      0.64      0.64       250\n",
            " comp.os.ms-windows.misc       0.68      0.70      0.69       250\n",
            "comp.sys.ibm.pc.hardware       0.66      0.66      0.66       250\n",
            "   comp.sys.mac.hardware       0.72      0.78      0.75       250\n",
            "          comp.windows.x       0.79      0.75      0.77       250\n",
            "            misc.forsale       0.79      0.74      0.76       250\n",
            "               rec.autos       0.82      0.88      0.85       250\n",
            "         rec.motorcycles       0.83      0.89      0.86       250\n",
            "      rec.sport.baseball       0.90      0.91      0.91       250\n",
            "        rec.sport.hockey       0.94      0.93      0.93       250\n",
            "               sci.crypt       0.88      0.88      0.88       250\n",
            "         sci.electronics       0.77      0.68      0.72       250\n",
            "                 sci.med       0.91      0.82      0.87       250\n",
            "               sci.space       0.85      0.82      0.84       250\n",
            "  soc.religion.christian       0.74      0.82      0.78       250\n",
            "      talk.politics.guns       0.72      0.82      0.77       250\n",
            "   talk.politics.mideast       0.90      0.84      0.87       250\n",
            "      talk.politics.misc       0.64      0.63      0.64       250\n",
            "      talk.religion.misc       0.52      0.36      0.42       250\n",
            "\n",
            "                accuracy                           0.77      5000\n",
            "               macro avg       0.77      0.77      0.76      5000\n",
            "            weighted avg       0.77      0.77      0.76      5000\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------\n",
            "Confusion Matrix:\n",
            " [[186   0   0   0   0   0   0   2   1   0   0   2   2   1   4  14   2   7\n",
            "    3  26]\n",
            " [  3 160  28  13   8  17   3   1   2   0   1   1   3   2   3   0   1   0\n",
            "    2   2]\n",
            " [  1  14 174  19   7  20   5   0   1   2   0   2   3   0   1   1   0   0\n",
            "    0   0]\n",
            " [  3   8  20 166  27   3   8   2   2   0   0   1   9   0   1   0   0   0\n",
            "    0   0]\n",
            " [  0   4   8  17 196   0   5   2   1   0   0   2   9   2   3   0   0   0\n",
            "    1   0]\n",
            " [  4  28  15   3   1 188   4   1   1   0   0   1   1   0   3   0   0   0\n",
            "    0   0]\n",
            " [  4   4   4  12  11   1 185   9   4   2   1   4   5   0   2   1   1   0\n",
            "    0   0]\n",
            " [  1   0   0   2   0   2   4 221   8   0   0   1   4   2   0   0   2   0\n",
            "    3   0]\n",
            " [  2   1   0   1   3   0   2  11 222   2   1   0   1   1   0   0   2   0\n",
            "    1   0]\n",
            " [  0   2   0   0   1   1   3   2   3 228   8   0   1   0   0   1   0   0\n",
            "    0   0]\n",
            " [  0   0   2   0   0   0   1   0   2   8 232   3   0   0   0   1   0   0\n",
            "    1   0]\n",
            " [  2   4   1   0   3   1   1   1   0   1   0 220   4   0   2   0   3   0\n",
            "    5   2]\n",
            " [  1  14   1  16  15   4   7   8   0   1   0   3 169   3   3   1   0   1\n",
            "    3   0]\n",
            " [  5   3   1   1   0   0   1   1   9   2   1   1   4 206   3   5   2   1\n",
            "    4   0]\n",
            " [  3   7   0   0   0   1   0   4   3   1   1   0   4   4 206   2   0   0\n",
            "   11   3]\n",
            " [ 12   1   1   0   1   0   1   0   2   2   2   1   0   1   1 205   2   1\n",
            "    4  13]\n",
            " [  1   0   0   0   0   1   2   1   3   1   0   4   0   0   0   0 206   0\n",
            "   21  10]\n",
            " [  4   0   0   0   0   0   2   3   2   0   0   2   0   0   0   4   4 211\n",
            "   15   3]\n",
            " [  3   0   0   1   0   0   0   1   2   2   0   2   0   3   6   1  38  10\n",
            "  157  24]\n",
            " [ 71   2   0   0   0   0   0   0   1   0   0   0   1   1   5  40  24   3\n",
            "   13  89]]\n",
            "------------------------------------------------------------------------------------------------------------------------------------\n",
            "Accuracy: 76.53999999999999 %\n"
          ]
        }
      ],
      "source": [
        "print('Classification Report:\\n', classification_report(Y_test, Y_preds))\n",
        "print('-'*132)\n",
        "print('Confusion Matrix:\\n', confusion_matrix(Y_test, Y_preds))\n",
        "print('-'*132)\n",
        "print('Accuracy:', accuracy_score(Y_test, Y_preds) * 100, '%')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Text Classification.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}